{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Pandas DF to Verseagility-JSON\n",
    "\n",
    "- Give a pandas dataframe as input with the columns \"label\" and \"text\" to bring it in the correct JSON-format\n",
    "- Just keep the other dummy variables as they are to make sure the data stays valid. You can just remove the question/answering component from your case file, so that this part does not get trained\n",
    "- You can also change the values for url, date etc. if you have a respective column in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import json\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to your format respectively\n",
    "fname = 'file.csv'\n",
    "df = pd.read_csv(fname, sep=\"\\t\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_json(df, language, chunk=0):\n",
    "    fname = f\"output-{language}-train-{chunk}.json\"\n",
    "    with open(fname, \"w\", encoding='utf-8') as file:\n",
    "        file.write(\"[\")\n",
    "    k = 0\n",
    "    for index, row in df.iterrows():\n",
    "        fileid = row['id']\n",
    "        q_views = row['views']\n",
    "        q_tags = row['appliesTo']\n",
    "        url = row['url']\n",
    "        lang = language\n",
    "\n",
    "        # PACK Q JSON\n",
    "        question = {}\n",
    "        question['title'] = row['question.title']\n",
    "        question['author'] = row['question.author']\n",
    "        question['createdAt'] = row['question.createdAt']\n",
    "        question['text'] = row['question.text']\n",
    "        question['upvotes'] = int(row['question.upvotes'])\n",
    "\n",
    "        # PACK A JSON\n",
    "        answer = {}\n",
    "        answer['markedAsAnswer'] = str(row['answer.markedAsAnswer'])\n",
    "        answer['createdAt'] = row['answer.createdAt']\n",
    "        answer['text'] = row['answer.text']\n",
    "        answer['upvotes'] = int(row['answer.upvotes'])\n",
    "\n",
    "        # PACK JSON\n",
    "        data = {'question': question, 'id': fileid, 'views': q_views, 'appliesTo': q_tags, 'url': url, 'language': lang, 'answer': answer}\n",
    "        content = json.dumps(data, indent=4, separators=(',', ': '), ensure_ascii=False)\n",
    "\n",
    "        # WRITE TO JSON FILE\n",
    "        with open(fname, \"a\", encoding='utf-8') as file:\n",
    "            if k == len(df) - 1: # cannot take index as it is read chunk-wise and therefore misleading\n",
    "                file.write(content + \"]\")\n",
    "            else:\n",
    "                file.write(content + \",\")\n",
    "        k = k + 1\n",
    "    try:\n",
    "        with open(fname) as f:\n",
    "            json.load(f)\n",
    "        logging.info(f'[INFO] - File {fname} is valid!')\n",
    "    except Exception as e:\n",
    "        logging.error(f'File {fname} seems to be invalid -> {e}.')\n",
    "    logging.info(f\"[SUCCESS] - File {chunk} -> {k} / {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate the transformation, if you want to transform the data set. Do this if you have less than 10.000 documents in your dataset\n",
    "transform_json(df, \"en-us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunk the data sets\n",
    "## Please note:\n",
    "- For training data corpora larger than 10.000 documents, we recommend to upload them chunk-wise to the BLOB-storage, otherwise it might come to bottlenecks in the document processor function\n",
    "    - Ãf you have less than 10.000 documents, you may go ahead and simply upload the file to the BLOB storage using the Azure Storage Explorer\n",
    "- The following section helps you to read a large file and split it into chunks\n",
    "- Below, there is a script to upload them one-by-one while having a break for five minutes to unload the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks(lang, language):\n",
    "    print(f'[INFO] - Start reading data chunks for {lang}.')\n",
    "    i = 0\n",
    "    for _ in pd.read_csv(f'data_{lang}.txt', sep=\"\\t\", encoding='utf-8', chunksize=5000):\n",
    "        transform_json(_, language, i)\n",
    "        i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_chunks(lang, language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copy to BLOB\n",
    "- Upload all the files of the export folder to the BLOB storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, ContentSettings\n",
    "from datetime import datetime\n",
    "import time\n",
    "import logging\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_to_blob(local_file_path, blobstring, container):\n",
    "    # Create a blob client using the local file name as the name for the blob\n",
    "    logging.info(f'[INFO] - Initiating upload to BLOB-storage.')\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(blobstring)\n",
    "    logging.info(f'[INFO] - Built connection to BLOB storage.')\n",
    "    for path, subdirs, files in os.walk(local_file_path):\n",
    "        for name in files:\n",
    "            try:\n",
    "                path_full = os.path.join(path, name)\n",
    "                path_blob = os.path.join(path, name).replace(local_file_path, \"\")\n",
    "                logging.info(f'[UPLOAD - {datetime.now()}] - Uploading to Azure Storage as BLOB: {path_blob}.')\n",
    "                blob_client = blob_service_client.get_blob_client(container=container, blob=path_blob)\n",
    "                # Upload the created file\n",
    "                with open(path_full, \"rb\") as data:\n",
    "                    blob_client.upload_blob(data, content_settings=ContentSettings(content_type='application/json'))\n",
    "                logging.info(f'[INFO - {datetime.now()}] - Upload completed, sleeping for 10 minutes ... zZz ...')\n",
    "                time.sleep(600)\n",
    "            except Exception as e:\n",
    "                logging.error(f'[STATUS - {datetime.now()}] - Copy to BLOB failed -> {e}.')\n",
    "    logging.info(f'[STATUS - {datetime.now()}] - Successfully uploaded to BLOB.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobstring = \"DefaultEndpointsProtocol=https;AccountName=###getyourblobstringhere###;AccountKey=###getyourkeyhere###;EndpointSuffix=core.windows.net\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_to_blob(f\"export-{lang}/\", blobstring, \"data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py36_automl",
   "language": "python",
   "name": "conda-env-azureml_py36_automl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}